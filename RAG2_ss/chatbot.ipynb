{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbde2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from mistralai import Mistral\n",
    "import chromadb\n",
    "import gc\n",
    "import time\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c600510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8842d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.environ[\"mistral_api_key\"]\n",
    "client=Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69187cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: {'C:\\\\Users\\\\USER\\\\Documents\\\\Lux_assign01\\\\RAG2_ss\\\\Finance_Bill_259(RAG).pdf'}\n",
      "VECTOR_DB_DIR: {'C:\\\\Users\\\\USER\\\\Documents\\\\Lux_assign01\\\\RAG2_ss\\\\chroma_db'}\n"
     ]
    }
   ],
   "source": [
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"Finance_Bill_259(RAG).pdf\"))\n",
    "DATA_DIR=os.path.join(SCRIPT_DIR, \"Finance_Bill_259(RAG).pdf\")\n",
    "VECTOR_DB_DIR=os.path.join(SCRIPT_DIR,\"chroma_db\")\n",
    "#avoiding hardcoding absolute paths that might break on other machines if you move your script by use of script_dir and os.path.join for code portability\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"PDF file not found at: {DATA_DIR}\")\n",
    "if not os.path.exists(VECTOR_DB_DIR):\n",
    "    os.makedirs(VECTOR_DB_DIR)\n",
    "\n",
    "print(f\"DATA_DIR:\",{DATA_DIR})\n",
    "print(f\"VECTOR_DB_DIR:\",{VECTOR_DB_DIR})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df09eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MAX_TOKENS = 256\n",
    "CHUNK_SIZE_TOKENS = 200 # A bit less than 256 to give buffer\n",
    "CHUNK_OVERLAP_TOKENS = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b565d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear_vector_store(directory, max_attempts=3, delay=2):\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Ensure no open connections\n",
    "            gc.collect()\n",
    "            if os.path.exists(directory):\n",
    "                shutil.rmtree(directory)\n",
    "                print(f\"Cleared directory: {directory}\")\n",
    "            return\n",
    "        except PermissionError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_attempts} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    raise PermissionError(f\"Failed to clear directory {directory} after {max_attempts} attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d2cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data():\n",
    "    print(f\"Loading documents from {DATA_DIR}...\")\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        print(f\"PDF file not found at {DATA_DIR}\")\n",
    "        raise FileNotFoundError(f\"PDF not found at {DATA_DIR}\")\n",
    "    \n",
    "    try:\n",
    "        loader=PyPDFLoader(DATA_DIR)\n",
    "        documents=loader.load()\n",
    "        print(f\"loaded {len(documents)} documents successfuly...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PDF: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Splitting documents into chunks...\")#splitting into chunks\n",
    "    #token-based splitting to respct model limits\n",
    "\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    tokenizer= AutoTokenizer.from_pretrained(model_name,token=os.getenv(\"hf_token_key\"))    \n",
    "    \n",
    "    def token_length(text):\n",
    "        return len(tokenizer.encode(text,add_special_tokens=True, truncation=False))#defining token length based on the embedding tokenizer\n",
    "        \n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE_TOKENS, #max size of each chunk\n",
    "        chunk_overlap=CHUNK_OVERLAP_TOKENS, #overlap between chunks to maintain context\n",
    "        length_function=token_length, #custom fxn\n",
    "        is_separator_regex=False, #standard separators\n",
    "    )\n",
    "\n",
    "    chunks=text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(chunks)} chunks sucessfully.\")\n",
    "    \n",
    "    #creating embeddings and vectordb storage.\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={\"device\":\"cpu\"},#were GPU available, \"cuda\" would be well-suited\n",
    "                                       encode_kwargs={\"normalize_embeddings\":True,\n",
    "                                                        \"truncate\": True, #ensure truncation if a chunk somehow still exceeds\n",
    "                                                        \"max_length\":EMBEDDING_MAX_TOKENS,\n",
    "                                                        \"batch_size\":32})#truncate to 512 texts\n",
    "    \n",
    "    print(\"Creating embeddings and instoring to ChromaDB...\")\n",
    "\n",
    "    #creating ChromaDB vector store to load the database in the specified directory\n",
    "    os.makedirs(VECTOR_DB_DIR,exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        valid_chunks=[]\n",
    "        for chunk in chunks:\n",
    "            tokens=token_length(chunk.page_content)\n",
    "            if tokens>EMBEDDING_MAX_TOKENS:\n",
    "                print(f\"Truncating chunk with {tokens} tokens\")\n",
    "                chunk.page_content=tokenizer.decode(tokenizer.encode(chunk.page_content,max_length=256,truncation=True))\n",
    "            valid_chunks.append(chunk)\n",
    "\n",
    "        print(f\"Number of valid chunks: {len(valid_chunks)}\")\n",
    "\n",
    "        chroma_client = chromadb.PersistentClient(\n",
    "            path=VECTOR_DB_DIR\n",
    "        )\n",
    "\n",
    "        vectorstore=Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=VECTOR_DB_DIR,\n",
    "            client=chroma_client\n",
    "        )\n",
    "\n",
    "        print(f\"Embeddings stored in {VECTOR_DB_DIR}\")\n",
    "        print(f\"Number of documents in vector store is:\",vectorstore._collection.count())\n",
    "\n",
    "        if vectorstore._collection.count()==0:\n",
    "            print(\"No documents stored in ChromaDB\")\n",
    "            raise ValueError(\"Ingestion failed:No documents stored!\")        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to store embeddings:{e}\")\n",
    "        raise\n",
    "\n",
    "    if vectorstore._collection.count()>0:\n",
    "        print(f\"Ingestion step successful to {VECTOR_DB_DIR} directory successfully...\")\n",
    "    else:\n",
    "        print(\"Ingestion step failed to store embeddings.ERROR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0095ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "VECTOR_DB_DIR=\"chroma_db/\" #directory where the ChromaDB is stored.\n",
    "\n",
    "def setup_rag_chain():\n",
    "    print(f\"Loading vector db from {VECTOR_DB_DIR}...\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={\"device\":\"cpu\"},\n",
    "                                       encode_kwargs={\"normalize_embeddings\":True,\n",
    "                                                        \"truncate\": True, \n",
    "                                                        \"max_length\":EMBEDDING_MAX_TOKENS,\n",
    "                                                        \"batch_size\":32})\n",
    "    \n",
    "    vectorstore=Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=VECTOR_DB_DIR\n",
    "    )#to load the existing chromaDB,note the chunks and their embeddings from the chroma\n",
    "\n",
    "    count=vectorstore._collection.count()\n",
    "    print(f\"Vector store contains {count} documents.\")\n",
    "\n",
    "    if count==0:\n",
    "        print(\"Vector store is empty! Ingest data first.\")\n",
    "        raise ValueError(\"Chroma database is empty!!!.\")\n",
    "    \n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\":3})#retrieves the top n relevant chunks.\n",
    "\n",
    "    print(\"Initializing LLM...\")#now initializing the llm\n",
    "    llm=ChatMistralAI(model=\"mistral-small-latest\",api_key=os.getenv(\"mistral_api_key\"),\n",
    "        max_tokens=200)\n",
    "    \n",
    "    # This is the formatting function\n",
    "    format_docs = (lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs))\n",
    "\n",
    "    #prompt engineering\n",
    "    template=(\"\"\"I am an AI assistant that answers your questions based off of the The Proposed Financ Bill 25/26.\n",
    "        context:{context}\n",
    "        question:{question}\n",
    "        Answer:\n",
    "        \"\"\")\n",
    "    prompt=ChatPromptTemplate.from_template(template)#this' the prompt dor context and question\n",
    "\n",
    "    rag_chain=(\n",
    "        {\"context\":retriever|format_docs,\n",
    "         \"question\":RunnablePassthrough()}\n",
    "         |prompt\n",
    "         |llm\n",
    "         |StrOutputParser()\n",
    "    )\n",
    "    print(\"RAG Chain setup complete!\")\n",
    "    return rag_chain\n",
    "\n",
    "rag_chain=None#Global variable to store the RAG chain\n",
    "\n",
    "def gradio_chat(user_input,history):\n",
    "    try:\n",
    "        response=rag_chain.invoke(user_input)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        error_msg=(f\"Error:{str(e)}\")\n",
    "        return error_msg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing vector db storage...\n",
      "Cleared directory: C:/Users/USER/Documents/Lux_assign01/RAG2_ss/chroma_db\n",
      "Running ingestion...\n",
      "Loading documents from C:\\Users\\USER\\Documents\\Lux_assign01\\RAG2_ss\\Finance_Bill_259(RAG).pdf...\n",
      "loaded 135 documents successfuly...\n",
      "Splitting documents into chunks...\n",
      "Split into 616 chunks sucessfully.\n",
      "Creating embeddings and instoring to ChromaDB...\n",
      "Number of valid chunks: 616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in chroma_db/\n",
      "Number of documents in vector store is: 616\n",
      "Ingestion step successful to chroma_db/ directory successfully...\n",
      "Loading vector db from chroma_db/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21024\\3301508702.py:30: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=400),\n",
      "C:\\Users\\USER\\Documents\\Lux_assign01\\RAG2_ss\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store contains 616 documents.\n",
      "Initializing LLM...\n",
      "RAG Chain setup complete!\n",
      "Launching Gradio chat UI...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://43c302a8c6f5d823ca.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://43c302a8c6f5d823ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__==\"__main__\":  \n",
    "    #clear vector storage\n",
    "    try:\n",
    "        print(\"Clearing vector db storage...\")\n",
    "        clear_vector_store(directory=\"C:/Users/USER/Documents/Lux_assign01/RAG2_ss/chroma_db\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearind vector db storage: {e}.\")\n",
    "        raise\n",
    "    \n",
    "    #run ingestion\n",
    "    try:\n",
    "        print(\"Running ingestion...\")\n",
    "        ingest_data()\n",
    "    except Exception as e:\n",
    "        print(f\"Error ingesting data: {e}\")\n",
    "        raise\n",
    "\n",
    "    #run RAG chain\n",
    "    try:\n",
    "        rag_chain=setup_rag_chain()\n",
    "    except Exception as e:\n",
    "        print(f\"Setup Error: {e}.!!!\")\n",
    "        raise\n",
    "\n",
    "    ## Step 4: Launching Gradio Chat\n",
    "    print(\"Launching Gradio chat UI...\")\n",
    "    gr.ChatInterface(fn=gradio_chat,\n",
    "                     title=\"ðŸ“˜ Finance Bill Chatbot\",\n",
    "                     description=\"Ask questions about the Finance Bill 2025/2026.\",\n",
    "                     chatbot=gr.Chatbot(height=400),\n",
    "                     theme=\"default\").launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
